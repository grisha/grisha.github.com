<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Grisha Trubetskoy]]></title>
  <link href="http://grisha.org/atom.xml" rel="self"/>
  <link href="http://grisha.org/"/>
  <updated>2015-05-04T20:23:19-04:00</updated>
  <id>http://grisha.org/</id>
  <author>
    <name><![CDATA[Gregory Trubetskoy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recording Time Series - Graphite vs RRDTool]]></title>
    <link href="http://grisha.org/blog/2015/05/04/recording-time-series/"/>
    <updated>2015-05-04T17:40:00-04:00</updated>
    <id>http://grisha.org/blog/2015/05/04/recording-time-series</id>
    <content type="html"><![CDATA[<p>This write up focuses on different methods of recording time series,
one by
<a href="http://graphite.readthedocs.org/en/latest/overview.html">Graphite</a>,
the other by <a href="https://oss.oetiker.ch/rrdtool/">RRDTool</a> and describes
the implications.</p>

<p>So a time series is simply a sequence of <code>(time, value)</code> tuples. The
most naive method of recording a time series is to store timestamps as
is. Since the data points might arrive at arbitrary and inexact
intervals, to correlate the series with a particular point in time
might be tricky. If datapoints are arriving somewhere in between one
minute bounaries (as they always naturally would), to answer the
question of what happened during a particular minute would require
specifying a range, which is not as clean as being able to specify a
precise value. To join two series on an imprecise timestemp is even trickier.</p>

<p>One way to improve upon this is to divide time into intervals of a
particular size and assign datapoints to the intervals. For example,
if our interval size is 10 seconds (I may sometimes refer to it as the
<em>step</em>), we could divide the entire timeline starting from the
<a href="http://en.wikipedia.org/wiki/Unix_time">beginning of the epoch</a> and
until the end of universe into 10 second slots. Since the first slot
begins at 0, any 10-second-step time series will have slots starting
at the exact same time. Now correlation across series or other time
values becomes much easier.</p>

<p>Calculating the slot is trivially easy: <code>time % step</code> (<code>%</code> being
<a href="https://docs.python.org/3.4/reference/expressions.html#index-51">modulo operator</a>).
But there is a complex subtelty lurking when it comes to assigning data points to slot.</p>

<p>Graphite, for example, just changes the timestamp of the datapoint to
the beginning of the slot.  If multiple data points arrive in the same
step, then the last one &#8220;wins&#8221;.</p>

<p>On the surface there is little wrong with this approach. In fact,
under right circumstances, there is absolutely nothing wrong with
it. Consider the following example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Graphite, 10 second step.
</span><span class='line'>
</span><span class='line'>Actual Time   Adjuste Time
</span><span class='line'>1430701282    1430701280     50  &lt;-- This datapoint is lost
</span><span class='line'>1430701288    1430701280     10
</span><span class='line'>1430701293    1430701290     30
</span><span class='line'>1430701301    1430701300     30</span></code></pre></td></tr></table></div></figure>


<p>Let&#8217;s pretend those values are some system metric like the number of
files open. The consequnce of the 50 being dropped is that we will
never know it existed, but towards the end of the 10 second interval
it went down to 10, which is still a true fact. If we really wanted to
know about the variations within a 10 second interval, we should have
chosen a smaller step, e.g. 1 second. By deciding that the step is
going to be 10 seconds, we thus declared that <em>variations within a
smaller period are of no interest</em> to us, and from this perspective,
Graphite <em>is correct</em>.</p>

<p>But what if those numbers are the price of a stock. There may be
hundreds of thousand of trades within a 10 second interval, yet we do
not want to (or cannot, for technical reasons) record every single one
of them? In this scenario having the last value override all previous
ones doesn&#8217;t exactly seem correct.</p>

<p>Enter RRDTool which uses a different method. RRDTool keeps track of
the last timestamp and calculates a weight for every incoming
datapoint based on time since last update or beginning of the step and
the step length. Here is what the same sequence of points looks like
in RRDTool. The lines marked with a <code>*</code> are not actual data points,
but are the last value for the preceeding step, it&#8217;s used for
computing the value for the remainder of the step after a new one has
begun.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>RRDTool, 10 second step.
</span><span class='line'>
</span><span class='line'>  Time          Value       Time since  Weight  Adjusted   Recorded
</span><span class='line'>                            last                value      value
</span><span class='line'>  1430701270    0           N/A
</span><span class='line'>* 1430701280    50         10s          1       50* 1= 50
</span><span class='line'>                                                           50
</span><span class='line'>  1430701282    50          2s          .2      50*.2= 10
</span><span class='line'>  1430701288    10          6s          .6      10*.6= 6
</span><span class='line'>* 1430701290    30          2s          .2      30*.2= 6
</span><span class='line'>                                                           10+6+6= 22
</span><span class='line'>  1430701293    30          3s          .3      30*.3= 9
</span><span class='line'>* 1430701300    30          7s          .7      30*.7= 21
</span><span class='line'>                                                           9+21=   30
</span><span class='line'>  1430701301    30   # this data point is incomplete</span></code></pre></td></tr></table></div></figure>


<p>So to compare the results of the two tools:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Time Slot     Graphite    RRDTool
</span><span class='line'>1430701270       N/A        50
</span><span class='line'>1430701280       10         22
</span><span class='line'>1430701290       30         30
</span><span class='line'>1430701300       N/A        N/A
</span></code></pre></td></tr></table></div></figure>


<p>Before you say &#8220;so what, I don&#8217;t really understand the difference&#8221;,
let&#8217;s pretend that those numbers were actually the rate of sale of
trinkets from our website (per second). Here is a horizontal ascii-art
rendition of our timeline, 0 is 1430701270.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0         10        20        30    time &gt;&gt;
</span><span class='line'>+.........+.........+.........+.....
</span><span class='line'>|           |     |    |       |
</span><span class='line'>0           50    10   30      30   data points</span></code></pre></td></tr></table></div></figure>


<p>At 1430701282 we recorded selling 50 trinkets per second. Assuming we
started selling at the beginning of our timeline, i.e. 12 seconds
earlier, we can state that during the first step we sold exactly 500
trinkets. Then 2 seconds into the second step we sold another 100
(we&#8217;re still selling at 50/s). Then for the next 6 seconds we were
selling at 10/s, thus another 60 trinkets, and for the last 2 seconds
of the slot we sold another 60 at 30/s. In the third step we were
selling steadily at 30/s, thus exactly 300 were sold.</p>

<p>Comparing RRDTool and Graphite side-by-side, the stories are quite different:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Trinkets per second:
</span><span class='line'>   Time Slot     Graphite Trinkets     RRDTool Trinkets
</span><span class='line'>1. 1430701270      N/A      N/A          50      500
</span><span class='line'>2. 1430701280       10      100          22      220 (100+60+60)
</span><span class='line'>3. 1430701290       30      300          30      300
</span><span class='line'>4. 1430701300       30      N/A          N/A     N/A
</span><span class='line'>                          -----                -----
</span><span class='line'>   TOTAL:                   400                 1020
</span></code></pre></td></tr></table></div></figure>


<p>Two important observations here:</p>

<ol>
<li>The totals are vastly different.</li>
<li>The rate recorded by RRDTool for the second slot (22/s), yields
exactly the number of trinkets sold during that period: 220.</li>
</ol>


<p>Last, but hardly the least, consider what happens when we consolidate
data points into larger intervals by averaging the values. Let&#8217;s say
20 seconds, twice our step. If we consolidate the second and the third
steps, we would get:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Graphite:  average(10,30) = 20  =&gt; 400 trinkets in 20 seconds
</span><span class='line'>RRDTool:   average(22,30) = 26  =&gt; 520 trinkets in 20 seconds</span></code></pre></td></tr></table></div></figure>


<p>Since the Graphite numbers were off to begin with, we have no reason
to trust the 400 trinkets number. But using the RRDTool, the new
number happens to still be 100% accurate even after the data points
have been consolidated. This is a very useful property of rates in
time series..</p>

<p>If you&#8217;re interested in learning more about this, I recommend reading
the documentation for <a href="http://oss.oetiker.ch/rrdtool/doc/rrdcreate.en.html">rrdtool
create</a>, in
particular the &#8220;It&#8217;s always a Rate&#8221; section, as well as <a href="http://www.vandenbogaerdt.nl/rrdtool/process.php">this post</a>
by Alex van den Bogaerdt.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Time Series]]></title>
    <link href="http://grisha.org/blog/2015/03/28/on-time-series/"/>
    <updated>2015-03-28T15:40:00-04:00</updated>
    <id>http://grisha.org/blog/2015/03/28/on-time-series</id>
    <content type="html"><![CDATA[<h2>Is it even a thing?</h2>

<p>Time Series is on its way to becoming a buzzword in the Information
Technology circles. This has to do with the looming Internet of Things
which shall cause the Great Reversal of Internet whereby upstream flow
of data produced by said Things is expected to exceed the downstream
flow. Much of this data is expected to be of the Time Series kind.</p>

<p>This, of course, is a money-making opportunity of the Big Data
proportions all over again, and I predict we&#8217;re going to see a lot of
Time Series support of various shapes and forms appearing in all
manners of (mostly commercial) software.</p>

<p>But is there really such a thing as the problem specifically inherent
to Time Series data which warrants a specialized solution? I&#8217;ve been
pondering this for some time now, and I am still undecided. This
here is my attempt at arguing that TS is <em>not</em> a special problem and
that it can be done by using a database like PostgreSQL.</p>

<h2>Influx of data and write speeds</h2>

<p>One frequently cited issue with time series data is that it arrives in
large volumes at a steady pace which renders buffered writes
useless. The number of incoming data streams can also be large
typically causing a disk seek per stream and further complicating the
write situation. TS data also has a property where often more data is
written than read because it&#8217;s possible for a datapoint to be
collected and examined only once, if ever. In short, TS is very
write-heavy.</p>

<p>But is this unique? For example logs have almost identical
properties. The real question here is whether our tried and true
databases such as PostgreSQL are ill-equipped to deal with large
volumes of incoming data requiring an alternative solution.</p>

<p>When considering incoming data I am tempted to imagine every US
household sending it, which, of course, would require massive
infrastructure. But this (unrealistic) scenario is not a TS data
problem, it&#8217;s one of scale, the same one from which the Hadoops and
Cassandras of this world were born. What is really happening here is
that TS happens to be yet another thing that requires the difficult to
deal with &#8220;big data&#8221; infrastructure and reiterates the need for an
easy-to-setup horizontally scalable database (which PostgreSQL isn&#8217;t).</p>

<h2>The backfill problem</h2>

<p>This is the problem of having to import vast amounts of historical
data. For example OpenTSDB goes to great lengths to optimize
back-filling by structuring it in specific ways and storing compressed
blobs of data.</p>

<p>But just like the write problem, it&#8217;s not unique to TS. It
is another problem that is becoming more and more pertinent as our
backlogs of data going back to when we stopped using paper keep
growing and growing.</p>

<h2>Downsampling</h2>

<p>Very often TS data is used to generate charts. This is an artifact of
the human brain being spectacularly good at interpreting a visual
representation of a relationship between streams of numbers while
nearly incapable of making sense of data in tabular form. When
plotting, no matter how much data is being examined, the end result is
limited to however many pixels are available on the display. Even
plotting aside, most any use of time series data is in an aggregated
form.</p>

<p>The process of consolidating datapoints into a smaller number (e.g.
the pixel width of the chart), sometimes called <em>downsampling</em>, involves
aggregation around a particular time interval or simply picking every
Nth datapoint.</p>

<p>As an aside, selecting every Nth row of a table is an interesting SQL
challenge, in PostgreSQL it looks like this (for every 100th row):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> SELECT time, data FROM
</span><span class='line'>   (SELECT *, row_number() OVER (ORDER BY time) as n FROM data_points) dp
</span><span class='line'>      WHERE dp.n % 100 = 0 ORDER BY time</span></code></pre></td></tr></table></div></figure>


<p>Aggregation over a time interval similar to how InfluxDB does it with
the <code>GROUP BY time(1d)</code> syntax can be easily achieved via the
<code>date_trunc('day', time)</code>.</p>

<p>Another aspect of downsampling is that since TS data is immutable,
there is no need to repeatedly recompute the consolidated version. It
makes more sense to downsample immediately upon the receipt of the
data and to store it permanently in this form. RRDTool&#8217;s Round-Robin
database is based entirely on this notion. InfluxDB&#8217;s continuous
queries is another way persistent downsampling is addressed.</p>

<p>Again, there is nothing TS-specific here. Storing data in summary form
is quite common in the data analytics world and a &#8220;continuous query&#8221;
is easily implemented via a trigger.</p>

<h2>Derivatives</h2>

<p>Sometimes the data from various devices exists in the form of a
counter, which requires the database to derive a rate by comparing
with a previous datapoint. An example of this is number of bytes sent
over a network interface. Only the rate of change of this value is
relevant, not the number itself. The rate of change is the difference
with the previous value divided over the time interval passed.</p>

<p>Referring to a previous row is also a bit tricky but perfectly doable
in SQL. It can accomplished by using windowing functions such as
<code>lag()</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SELECT time,
</span><span class='line'>  (bytes - lag(bytes, 1) OVER w) / extract(epoch from (time - lag(time, 1) OVER w))::numeric
</span><span class='line'>    AS bytes_per_sec
</span><span class='line'>  FROM data_points
</span><span class='line'>  WINDOW w AS (ORDER BY time)
</span><span class='line'>  ORDER BY time</span></code></pre></td></tr></table></div></figure>


<h2>Expiration</h2>

<p>It is useful to downsample data to a less granular form as it ages,
aggregating over an ever larger period of time and possibly purging
records eventually. For example we might want to store minutely data
for a week, hourly for 3 months, daily for 3 years and drop all data
beyond 3 years.</p>

<p>Databases do not expire rows &#8220;natively&#8221; like Cassandra or Redis, but it
shouldn&#8217;t be too hard to accomplish via some sort of a periodic cron
job or possibly even just triggers.</p>

<h2>Heartbeat and Interval Filling</h2>

<p>It is possible for a time series stream to pause, and this can be
interpreted in different ways: we can attempt to fill in missing data,
or treat it as unknown. More likely we&#8217;d want to start treating it as
unknown after some period of silence. RRDTool addresses this by
introducing the notion of a <em>heartbeat</em> and the number of missed beats
before data is treated as unknown.</p>

<p>Regardless of whether the value is unknown, it is useful to be able to
fill in a gap (missing rows) in data. In PostgreSQL this can be
accomplished by a join with a result set from the <code>generate_series()</code>
function.</p>

<h2>Data Seclusion</h2>

<p>With many specialized Time Series tools the TS data ends up being
secluded in a separate system not easily accessible from the rest of
the business data. You cannot join your customer records with data in
RRDTool or Graphite or InfluxDB, etc.</p>

<h2>Conclusion</h2>

<p>If there is a problem with using PosgreSQL or some other database for
Time Series data, it is mainly that of having to use advanced SQL
syntax and possibly requiring some cookie-cutter method for managing
Time Series, especially when it is a large number or series and high
volume.</p>

<p>There is also complexity in horizontally scaling a relational database
because it involves setting up replication, sharding, methods for
recovery from failure and balancing the data. But these are not
TS-specific problems, they are scaling problems.</p>

<p>Having written this up, I&#8217;m inclined to think that perhaps there is
no need for a specialized &#8220;Time Series Database&#8221;, instead it can be
accomplished by an application which uses a database for storage and
abstracts the users from the complexities of SQL and potentially even
scaling, while still allowing for direct access to the data via the
rich set of tools that a database like PostgreSQL provides.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How InfluxDB Stores Data]]></title>
    <link href="http://grisha.org/blog/2015/03/20/influxdb-data/"/>
    <updated>2015-03-20T15:52:00-04:00</updated>
    <id>http://grisha.org/blog/2015/03/20/influxdb-data</id>
    <content type="html"><![CDATA[<p>A nice, reliable, horizontally scalable database that is designed
specifically to tackle the problem of Time Series data (and does not
require you to stand up a Hadoop cluster) is very much missing from the
Open Source Universe right now.</p>

<p><a href="https://github.com/influxdb/influxdb">InfluxDB</a> might be able to fill this gap, it certainly aims to.</p>

<p>I was curious about how it structures and stores data and since there
wasn&#8217;t much documentation on the subject and I ended up just reading
the code, I figured I&#8217;d write this up. I only looked at the new
(currently 0.9.0 in RC stage) version, the previous versions are
significantly different.</p>

<p>First of all, InfluxDB is distributed. You can run one node, or a
bunch, it seems like a more typical number may be 3 or 5. The nodes
use <a href="https://github.com/goraft/raft">Raft</a> to establish consensus and maintain data consistency.</p>

<p>InfluxDB feels a little like a relational database in some aspects
(e.g. it has a SQL-like query language) but not in others.</p>

<p>The top level container is a <em>database</em>. An InfluxDB database is very
much like what a database is in MySQL, it&#8217;s a collection of other
things.</p>

<p>&#8220;Other things&#8221; are called <em>data points</em>, <em>series</em>, <em>measurements</em>,
<em>tags</em> and <em>retention policies</em>. Under the hood (i.e. you never deal
with them directly) there are <em>shards</em> and <em>shard groups</em>.</p>

<p>The very first thing you need to do in InfluxDB is create a database
and at least one retention policy for this database. Once you have
these two things, you can start writing data.</p>

<p>A retention policy is the time period after which the data expires. It
can be set to be infinite. A data point, which is a measurement
consisting of any number of values and tags associated with a
particular point in time, must be associated with a database and a
retention policy. A retention policy also specifies the <em>replication
factor</em> for the data point.</p>

<p>Let&#8217;s say we are tracking disk usage across a whole bunch of
servers. Each server runs some sort of an agent which periodically
reports the usage of each disk to InfluxDB. Such a report might look
like this (in JSON):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{"database" : "foo", "retentionPolicy" : "bar",
</span><span class='line'> "points" : [
</span><span class='line'>   {"name" : "disk",
</span><span class='line'>    "tags" : {"server" : "bwi23", "unit" : "1"},
</span><span class='line'>    "timestamp" : "2015-03-16T01:02:26.234Z",
</span><span class='line'>    "fields" : {"total" : 100, "used" : 40, "free" : 60}}]}</span></code></pre></td></tr></table></div></figure>


<p>In the above example, &#8220;disk&#8221; is a measurement. Thus we can operate on
anything &#8220;disk&#8221;, regardless of what &#8220;server&#8221; or &#8220;unit&#8221; it applies
to. The data point as a whole belongs to a (time) series identified by
the combination of the measurement name and the tags.</p>

<p>There is no need to create series or measurements, they are created on
the fly.</p>

<p>To list the measurements, we can use <code>SHOW MEASUREMENTS</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; show measurements
</span><span class='line'>name            tags    name
</span><span class='line'>----            ----    ----
</span><span class='line'>measurements            disk</span></code></pre></td></tr></table></div></figure>


<p>We can use <code>SHOW SERIES</code> to list the series:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; show series
</span><span class='line'>name    tags    id      server   unit
</span><span class='line'>----    ----    --      -------  ----
</span><span class='line'>disk            1       bw123    1</span></code></pre></td></tr></table></div></figure>


<p>If we send a record that contains different tags, we automatically
create a different series (or so it seems), for example if we send
this (note we changed &#8220;unit&#8221; to &#8220;foo&#8221;):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{"database" : "foo", "retentionPolicy" : "bar",
</span><span class='line'> "points" : [
</span><span class='line'>   {"name" : "disk",
</span><span class='line'>    "tags" : {"server" : "bwi23", "foo" : "bar"},
</span><span class='line'>    "timestamp" : "2015-03-16T01:02:26.234Z",
</span><span class='line'>    "fields" : {"total" : 100, "used" : 40, "free" : 60}}]}</span></code></pre></td></tr></table></div></figure>


<p>we get</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; show series
</span><span class='line'>name    tags    id      foo     server  unit
</span><span class='line'>----    ----    --      ---     ------  ----
</span><span class='line'>disk            1               bwi23   1
</span><span class='line'>disk            2       bar     bwi23</span></code></pre></td></tr></table></div></figure>


<p>This is where the distinction between measurement and series becomes a
little confusing to me. In actuality (from looking at the code and the
actual files InfluxDB created) there is only one series here called
&#8220;disk&#8221;. I understand the intent, but not sure that <em>series</em> is the
right terminology here. I think I&#8217;d prefer if measurements were simply
called series, and to get the equivalent of <code>SHOW SERIES</code> you&#8217;d use
something like <code>SHOW SERIES TAGS</code>. (May be I&#8217;m missing something.)</p>

<p>Under the hood the data is stored in shards, which are grouped by
shard groups, which in turn are grouped by retention policies, and
finally databases.</p>

<p>A database contains one or more retention policies. Somewhat
surprisingly a retention policy is actually a bucket. It makes sense
if you think about the problem of having to expire data points - you
can remove them all by simply dropping the entire bucket.</p>

<p>If we declare a retention policy of 1 day, then we can logically
divide the timeline into a sequence of single days from beginning of
the epoch. Any incoming data point falls into its corresponding
segment, which is a retention policy bucket. When clean up time comes
around, we can delete all days except for the most current day.</p>

<p>To better understand the following paragraphs, consider that having
multiple nodes provides the option for two things: <em>redundancy</em> and
<em>distribution</em>. Redundancy gives you the ability to lose a node
without losing any data. The number of copies of the data is
controlled by the replication factor specified as part of the
retention policy. Distribution spreads the data across nodes which
allows for concurrency: data can be written, read and processed in
parallel. For example if we become constrained by write performance,
we can solve this by simply adding more nodes. InfluxDB favors
redundancy over distribution when having to choose between the two.</p>

<p>Each retention policy bucket is further divided into shard groups, one
shard group per series. The purpose of a shard group is to balance
series data across the nodes of the cluster. If we have a cluster of 3
nodes, we want the data points to be evenly distributed across these
nodes. InfluxDB will create 3 shards, one on each of the nodes. The 3
shards comprise the shard group. This is assuming the replication
factor is 1.</p>

<p>But if the replication factor was 2, then there needs to be 2
identical copies of every shard. The shard copies must be on separate
nodes. With 3 nodes and replication factor of 2, it is impossible to
do any distribution across the nodes - the shard group will have a
size of 1, and contain 1 shard, replicated across 2 nodes. In this set
up, the third node will have no data for this particular retention
policy.</p>

<p>If we had a cluster of 5 nodes and the replication factor of 2, then
the shard group can have a size of 2, for 2 shards, replicated across
2 nodes each. Shard one replicas could live on nodes 1 and 3, while
shard two replicas on nodes 2 and 4. Now the data is distributed as
well as redundant. Note that the 5th node doesn&#8217;t do anything. If we
up the replication factor to 3 then just like before, the cluster is
too small to have any distribution, we only have enough nodes for
redundancy.</p>

<p>As of RC15 distributed queries are not yet implemented, so you will
always get an error if you have more than one shard in a group.</p>

<p>The shards themselves are instances of <a href="https://github.com/boltdb/bolt">Bolt db</a> - a simple to use key/value store
written in Go. There is also a separate Bolt db file called meta which
stores the metadata, i.e. information about databases, retention
policies, measurements, series, etc.</p>

<p>I couldn&#8217;t quite figure out the process for typical cluster operations
such as recovery from node failure or what happens (or should happen)
when nodes are added to existing cluster, whether there is a way to
decommission a node or re-balance the cluster similar to the Hadoop
balancer, etc. I think as of this writing this has not been fully
implemented yet, and there is no documentation, but hopefully it&#8217;s
coming soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby, HiveServer2 and Kerberos]]></title>
    <link href="http://grisha.org/blog/2014/08/19/ruby_hiveserver2_and_kerberos/"/>
    <updated>2014-08-19T08:03:00-04:00</updated>
    <id>http://grisha.org/blog/2014/08/19/ruby_hiveserver2_and_kerberos</id>
    <content type="html"><![CDATA[<p>Recently I found myself needing to connect to HiveServer2 with
Kerberos authentication enabled from a Ruby app. As it turned out
<a href="https://github.com/forward3d/rbhive">rbhive gem</a> we were using did not have
support for 