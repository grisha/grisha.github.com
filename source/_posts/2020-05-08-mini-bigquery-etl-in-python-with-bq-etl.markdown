---
layout: post
title: "Keeping ETL in BigQuery with bq_etl."
date: 2020-05-08 12:22
comments: true
published: false
categories:
---

When doing data processing that requires data to be local to the
machine, e.g. model training on a GPU, we often find ourselves piecing
together datasets from tables in BigQuery. (Or similar SQL-centric
environments such as Hadoop/Hive: the
[project](https://github.com/grisha/bq_etl) I describe here is
currently BigQuery-specific, but the concept applies and can be
extended to any such tool).

There is always the question of when does one bring data out of
BigQuery and how much more data transformation is acceptible
afterwards. I believe that the correct answer is a simple "None". All
possible data transformation should be done *before* the data leaves
BigQuery in my (rather strong) opinion.

But reality is oft otherwise. Once the data is in local storage, it is
much easier to load a dataset into a dataframe and do something with
it. Even though the proper fix would be to correct the SQL and run it
again, it is just so much simpler to tweak data locally. If data has
been corrected locally, it needs to be saved somewhere, so we end up
with files stored on disk and perhaps pushed out to GCS (or S3) for
preservation. Then, as progress is being made, and different data
transformations are preformed, we end up with multiple versions of
these files, each generated by different code, and it is important to
carefully keep track of which files the result of what version of
code. You can hopefully see how this is gradually becoming a data
nightmare.

This type of data monkey patching is difficult to track because the
place in code where it happens usually is not in any way tied to the
SQL statement which it is correcting. The SQL defining the original
data may be in a different code base or in none at all.

I'd like to propose my theory on why that is and propose a simple
solution.

First, it is not uncommon for whoever is running this task to be more
comfortable with Python/Pandas than BigQuery SQL. For example
renumbering items sequentially is as simple as creating a
`range(len(df))` column while in SQL it requires knowledge of
[Numbering functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/numbering_functions)
and understanding the "Resources exceeded" caveat.

A more important reason is the difficulty of scripting an SQL
statement and getting the data out of BigQuery. It is entirely likely
that the SQL statement is a result of a Python program which was not
intended to be executed piecemeal and running it again would result in
way more data being processed than seems reasonable. It is also
possible that the transfer of the data was not scripted at all and is
done by hand.

Here is my simple solution. It starts with a few requirements:

>  * Each SQL statement should be in its own .sql file.

This makes it possible to view and edit this code as what it is - SQL,
not a string in Python code.

>  * The name and the content of the .sql file should dictate the name
>    of the output table.

The simplest way to do this is with a short hash of the contents
appended to the table name. Once that is true, the .sql file becomes a
complete description of the output table. If the SQL file is altered,
it would result in a different table.

>  * There should be no need to use external tools to keep track of
>    the state.

You do not need MLFlow or Redis to know what the SQL has been
executed - the mere table's existence in BigQuery is evidence of it.

>  * The location of  any GCS extracts from this table should uniquely match the table name.

If the blob(s) exist, it means that the extract has been performed.

>  * The local filename should still depend on the original .sql file.

Which makes it easy to check whether the data has been downloaded to
local storage.

Note that given the above scheme, if the name or the content of the
.sql file changes, so does the table name, the GCS extract name and
the local file name.

Given the above few constraints, we can now answer the questions of
"Should this SQL be executed or has it already been done?", "Should
this table be extracted to GCS?" and "Should this GCS data be
downloaded?" and optionally forego these steps if they are already
done.

>  * The order of execution of .sql files should be inferred from the
>    SQL itself.

Last but no the least, if we have multiple SQL statements to be
executed, what dictates the order of execution. It turns out that SQL
statements do not need to be executed in any particular order *unless*
the output of one is input to another. As I have [written about before](/blog/2016/11/14/table-names-from-sql/),
this is best inferred
from the statement itself and does not need to be externally
specified. (In fact, I'm not sure why "Give me a bunch of SQL and I
will run it in correct order" is not a standard feature in big data
SQL platforms such as BigQuery).

Last week I put together a simplistic little package that does all of
the above. See it on Github at [https://github.com/grisha/bq_etl](https://github.com/grisha/bq_etl).

It was written to serve my specific needs and for this reason it is
very bare-bones, but that's not to say I'm not open to comments,
issues and pull requests to make it better!
