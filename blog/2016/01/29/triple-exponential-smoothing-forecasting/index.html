
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Holt-Winters Forecasting for Dummies (or Developers) - Part I - Gregory Trubetskoy</title>
  <meta name="author" content="Gregory Trubetskoy">

  
  <meta name="description" content="This three part write up [Part II
Part III]
is my attempt at a down-to-earth explanation (and Python code) of the
Holt-Winters method for those of us &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Gregory Trubetskoy" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="https://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
</script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-42971867-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Gregory Trubetskoy</a></h1>
  
    <h2>Notes to self.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:grisha.org" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Holt-Winters Forecasting for Dummies (or Developers) - Part I</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-01-29T15:36:00-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2016</time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>This three part write up [<a href="/blog/2016/02/16/triple-exponential-smoothing-forecasting-part-ii/">Part II</a>
<a href="/blog/2016/02/17/triple-exponential-smoothing-forecasting-part-iii/">Part III</a>]
is my attempt at a down-to-earth explanation (and Python code) of the
Holt-Winters method for those of us who while hypothetically might be
quite good at math, still try to avoid it at every opportunity. I had
to dive into this subject while tinkering on
<a href="https://github.com/tgres/tgres">tgres</a> (which features a Golang implementation). And
having found it somewhat complex (and yet so brilliantly
simple), figured that it’d be good to share this knowledge, and
in the process, to hopefully solidify it in my head as well.</p>

<p><a href="https://en.wikipedia.org/wiki/Exponential_smoothing#Triple_exponential_smoothing">Triple Exponential Smoothing</a>,
also known as the Holt-Winters method, is one of the many methods or
algorithms that can be used to forecast data points in a series,
provided that the series is “seasonal”, i.e. repetitive over some
period.</p>

<p><img src="/images/hw00.png" /></p>

<h1 id="a-little-history">A little history</h1>

<p>Еxponential smoothing in some form or another dates back to the work
of <a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson">Siméon Poisson</a> (1781-1840),
while its application in forecasting appears to have been pioneered over a century later in 1956 by
<a href="https://en.wikipedia.org/wiki/Robert_Goodell_Brown">Robert Brown</a> (1923–2013)
in his publication
<a href="https://industrydocuments.library.ucsf.edu/tobacco/docs/#id=jzlc0130">Exponential Smoothing for Predicting Demand</a>,
(Cambridge, Massachusetts). [Based on the URL it seems Brown was working on forecasting tobacco demand?]</p>

<p>In 1957 an <a href="http://web.mit.edu/">MIT</a> and <a href="http://www.uchicago.edu/">University of Chicago</a>
graduate, professor <a href="https://en.wikipedia.org/wiki/Charles_C._Holt">Charles C Holt</a>
(1921-2010) was working at <a href="http://www.cmu.edu/">CMU</a> (then known as CIT) on forecasting trends in production,
inventories and labor force.
It appears that Holt and Brown worked independently  and knew not of each-other’s work.
Holt published a paper “Forecasting trends
and seasonals by exponentially weighted moving averages” (Office of Naval Research Research
Memorandum No. 52, Carnegie Institute of Technology) describing
double exponential smoothing. Three years later, in 1960, a student of
Holts (?) Peter R. Winters improved the algorithm by adding seasonality and
published
<a href="http://pubsonline.informs.org/doi/abs/10.1287/mnsc.6.3.324">Forecasting sales by exponentially weighted moving averages</a>
(Management Science 6, 324–342), citing Dr. Holt’s 1957 paper as earlier work on the same subject.
This algorithm became known as triple exponential smoothing or the Holt-Winters method,
the latter probably because it was described in a 1960 Prentice-Hall book “Planning Production, Inventories, and Work Force”
by Holt, <a href="https://en.wikipedia.org/wiki/Franco_Modigliani">Modigliani</a>, <a href="https://en.wikipedia.org/wiki/John_Muth">Muth</a>,
<a href="https://en.wikipedia.org/wiki/Herbert_A._Simon">Simon</a>,
<a href="https://www.gsb.stanford.edu/faculty-research/faculty/charles-puis-bonini">Bonini</a> and Winters - good luck finding a copy!</p>

<p>Curiously, I’ve not been able to find any personal information on Peter R. Winters online. If you find anything, please let me
know, I’ll add a reference here.</p>

<p>In 2000 the Holt-Winters method became well known in the <a href="https://en.wikipedia.org/wiki/Internet_service_provider">ISP</a>
circles at the height of the <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">.com boom</a> when Jake D. Brutlag (then of WebTV) published
<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiAx9SNoezKAhXCC5oKHZZ4A84QFgghMAA&amp;url=https%3A%2F%2Fwww.usenix.org%2Fevents%2Flisa00%2Ffull_papers%2Fbrutlag%2Fbrutlag.pdf&amp;usg=AFQjCNEg-ynB5Ok0Sf4ATBB77PcGwT4OLw&amp;bvm=bv.113943665,d.bGs">Aberrant Behavior Detection in Time Series for Network Monitoring</a>
(Proceedings of the 14th Systems Administration Conference, LISA
2000). It described how an open source <a href="https://en.wikipedia.org/wiki/C_%28programming_language%29">C</a>
implementation [<a href="https://github.com/oetiker/rrdtool-1.x/commit/cafbbce69d9d1d4a1772299d97138b5a81d343f5">link to the actual commit</a>]
of a variant of the Holt-Winters seasonal method, which he contributed as a feature
to the very popular at ISPs <a href="http://oss.oetiker.ch/rrdtool/">RRDTool</a>, could be used to
monitor network traffic.</p>

<p>In 2003, a remarkable 40+ years since the publication of Winters
paper, professor <a href="http://www.sbs.ox.ac.uk/community/people/james-taylor">James W Taylor</a>
of <a href="http://www.ox.ac.uk/">Oxford University</a> extended the
Holt-Winters method to multiple seasonalities (i.e. $n$-th exponential
smoothing) and published <a href="http://users.ox.ac.uk/~mast0315/ExpSmDoubleSeasonal.pdf">Short-term electricity demand forecasting using double seasonal exponential smoothing</a>
(Journal of Operational
Research Society, vol. 54, pp. 799–805). (But we won’t cover Taylors
method here).</p>

<p>In 2011 the RRDTool implementation contributed by Brutlag was
<a href="https://github.com/graphite-project/graphite-web/commit/4e7a0d664ea2153ea65173138ab8f337716e21fa">ported</a>
to <a href="http://graphite.readthedocs.org/en/latest/">Graphite</a> by Matthew Graham thus making it even more popular in the
devops community.</p>

<p>So… how does it work?</p>

<h1 id="forecasting-baby-steps">Forecasting, Baby Steps</h1>

<p>The best way to explain triple exponential smoothing is to gradually
build up to it starting with the simplest forecasting methods. Lest
this text gets too long, we will stop at triple exponential smoothing,
though there are quite a few other methods known.</p>

<p>I used mathematical notation only where I thought it made best sense, sometimes
accompanied by an “English translation”, and where appropriate
supplemented with a bit of <a href="http://www.python.org">Python</a> code.
In Python I refrain from using any non-standard packages, keeping the
examples plain. I chose not to use <a href="https://wiki.python.org/moin/Generators">generators</a>
for clarity. The objective here is to explain
the inner working of the algorithm so that you can implement it
yourself in whatever language you prefer.</p>

<p>I also hope to demonstrate that this is simple enough that you do not
need to resort to <a href="http://www.scipy.org/">SciPy</a> or <a href="https://en.wikipedia.org/wiki/R_%28programming_language%29">whatever</a>
(not that there is anything wrong with that).</p>

<h2 id="but-first-some-terminology">But First, Some Terminology</h2>

<h3 id="series"><em>Series</em></h3>

<p>The main subject here is a <em>series</em>. In the real world we are most
likely to be applying this to a <em>time series</em>, but for this discussion
the time aspect is irrelevant. A series is merely an ordered sequence
of numbers. We might be using words that are chronological in nature
(past, future, yet, already, <em>time</em> even!), but only because it makes it easer to
understand. So forget about time, timestamps, intervals,
<a href="http://www.preposterousuniverse.com/blog/2013/10/18/is-time-real/">time does not exist</a>,
the only property each data point has (other than the value) is its order: first,
next, previous, last, etc.</p>

<p>It is useful to think of a series as a list of two-dimensional $x,y$
coordinates, where $x$ is order (always going up by 1), and $y$ is
value. For this reason in our math formulas we will be sticking to $y$
for value and $x$ for order.</p>

<h3 id="observed-vs-expected"><em>Observed</em> vs <em>Expected</em></h3>

<p>Forecasting is estimating values that we do not yet know based on the
the values we do know. The values we know are referred to as
<em>observed</em> while the values we forecast as <em>expected</em>. The math
convention to denote expected values is with the
<a href="https://en.wikipedia.org/wiki/Circumflex">circumflex</a> a.k.a. “hat”: $\hat{y}$</p>

<p>For example, if we have a series that looks like <code>[1,2,3]</code>, we might
forecast the next value to be 4. Using this terminology, given
observed series <code>[1,2,3]</code> the next expected value ${\hat{y}_4}$ is 4.</p>

<h3 id="method"><em>Method</em></h3>

<p>We may have intuited based on <code>[1,2,3]</code> that in this series each value
is 1 greater than the previous, which in math notation can
be expressed as and $\hat{y}_{x + 1} = y_x + 1$.  This equation, the
result of our intuition, is known as a forecast <em>method</em>.</p>

<p>If our method is correct then the next observed value would indeed be
4, but if <code>[1,2,3]</code> is actually part of a
<a href="https://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci sequence</a>, then where we
expected ${\hat{y}_4 = 4}$, we would observe $y_4 = 5$. Note the hatted
${\hat{y}}$ (expected) in the former and $y$ (observed) in the latter expression.</p>

<h3 id="error-sse-and-mse"><em>Error</em>, <em>SSE</em> and <em>MSE</em></h3>

<p>It is perfectly normal to compute expected values where we already
have observed values. Comparing the two lets you compute the <em>error</em>,
which is the <em>difference</em> between observed and expected and is an
indispensable indication of the accuracy of the method.</p>

<p>Since difference can be negative or positive, the common convention is
to use the absolute value or square the error so that the number is always
positive. For a whole series the squared errors are typically summed
resulting in <em><a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares">Sum of Squared Errors</a> (SSE)</em>.
Sometimes you may come across <em><a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares">Mean Squared Error</a>
(MSE)</em> which is simply $\sqrt{SSE}$.</p>

<h2 id="and-now-the-methods-where-the-fun-begins">And Now the Methods (where the fun begins!)</h2>

<p>In the next few examples we are going to be using this tiny series:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">series</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>(Feel free to paste it and any of the following code snippets into your Python
<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">repl</a>)</p>

<h3 id="naive-method">Naive Method</h3>

<p>This is the most primitive forecasting method. The premise of the
<em>naive</em> method is that the expected point is equal to the last
observed point:</p>

<script type="math/tex; mode=display">
\hat{y}_{x+1} = y_x
</script>

<p>Using this method we would forecast the next point to be 12.</p>

<h3 id="simple-average">Simple Average</h3>

<p>A less primitive method is the <a href="https://en.wikipedia.org/wiki/Arithmetic_mean">arithmetic average</a>
of all the previously observed data points. We take all the values we
know, calculate the average and bet that that’s going to be the next value. Of course it won’t be it exactly,
but it probably will be somewhere in the ballpark, hopefully you can see the reasoning behind this
simplistic approach.</p>

<script type="math/tex; mode=display">
\hat{y}_{x+1} = \dfrac{1}{x}\sum_{i=1}^{x}y_i
</script>

<p>(Okay, this formula is only here because I think the <a href="https://en.wikipedia.org/wiki/Summation">capital Sigma</a>
looks cool. I am sincerely hoping that the average requires no explanation.) In Python:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">average</span><span class="p">(</span><span class="n">series</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">series</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># Given the above series, the average is:</span>
</span><span class="line"><span class="c"># &gt;&gt;&gt; average(series)</span>
</span><span class="line"><span class="c"># 10.285714285714286</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>As a forecasting method, there are actually situations where it’s spot
on. For example your final school grade may be the average of all the
previous grades.</p>

<h3 id="moving-average">Moving Average</h3>

<p>An improvement over simple average is the average of $n$ last
points. Obviously the thinking here is that only the recent values
matter. Calculation of the moving average involves what is sometimes
called a “sliding window” of size $n$:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># moving average using n last points</span>
</span><span class="line"><span class="k">def</span> <span class="nf">moving_average</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">average</span><span class="p">(</span><span class="n">series</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:])</span>
</span><span class="line">
</span><span class="line"><span class="c"># &gt;&gt;&gt; moving_average(series, 3)</span>
</span><span class="line"><span class="c"># 11.333333333333334</span>
</span><span class="line"><span class="c"># &gt;&gt;&gt; moving_average(series, 4)</span>
</span><span class="line"><span class="c"># 11.75</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>A moving average can actually be quite effective, especially if you
pick the right $n$ for the series. Stock analysts adore it.</p>

<p>Also note that simple average is a variation of a moving average, thus
the two functions above could be re-written as a single recursive one
(just for fun):</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">average</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
</span><span class="line">    <span class="k">if</span> <span class="n">n</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">average</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">series</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:]))</span><span class="o">/</span><span class="n">n</span>
</span><span class="line">
</span><span class="line"><span class="c"># &gt;&gt;&gt; average(series, 3)</span>
</span><span class="line"><span class="c"># 11.333333333333334</span>
</span><span class="line"><span class="c"># &gt;&gt;&gt; average(series)</span>
</span><span class="line"><span class="c"># 10.285714285714286</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="weighted-moving-average">Weighted Moving Average</h3>

<p>A <em>weighted</em> moving average is a moving average where within the
sliding window values are given different weights, typically so that
more recent points matter more.</p>

<p>Instead of selecting a window size, it requires a list of weights
(which should add up to 1). For example if we picked <code>[0.1,
0.2, 0.3, 0.4]</code> as weights, we would be giving 10%, 20%, 30% and 40%
to the last 4 points respectively. In Python:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># weighted average, weights is a list of weights</span>
</span><span class="line"><span class="k">def</span> <span class="nf">weighted_average</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span class="line">    <span class="n">weights</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>
</span><span class="line">        <span class="n">result</span> <span class="o">+=</span> <span class="n">series</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
</span><span class="line">    <span class="k">return</span> <span class="n">result</span>
</span><span class="line">
</span><span class="line"><span class="c"># &gt;&gt;&gt; weights = [0.1, 0.2, 0.3, 0.4]</span>
</span><span class="line"><span class="c"># &gt;&gt;&gt; weighted_average(series, weights)</span>
</span><span class="line"><span class="c"># 11.5</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Weighted moving average is fundamental to what follows, please take a
moment to understand it, give it a think before reading on.</p>

<p>I would also like to stress the importance of the weights adding up
to 1. To demonstrate why, let’s say we pick weights <code>[0.9, 0.8, 0.7,
0.6]</code> (which add up to 3.0). Watch what happens:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&gt;&gt;&gt; weighted_average(series, [0.9, 0.8, 0.7, 0.6])
</span><span class="line">&gt;&gt;&gt; 35.5  # &lt;--- this is clearly bogus</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="picture-time">Picture time!</h3>

<p>Here is a picture that demonstrates our tiny series and all of the above
forecasts (except for naive).</p>

<p><img src="/images/hw01.png" /></p>

<p>It’s important to understand that which of the above methods is better
very much depends on the nature of the series. The order in which I
presented them was from simple to complex, but “more complex” doesn’t
necessarily mean “better”.</p>

<h3 id="single-exponential-smoothing">Single Exponential Smoothing</h3>

<p>Here is where things get interesting. Imagine a weighted average where
we consider <em>all</em> of the data points, while assigning exponentially
smaller weights as we go back in time. For example if we started with
0.9, our weights would be (going back in time):</p>

<script type="math/tex; mode=display">
0.9^1, 0.9^2, 0.9^3, 0.9^4, 0.9^5, 0.9^6... \\
\mbox{or: } 0.9, 0.81, 0.729, 0.6561, 0.59049, 0.531441, ...
</script>

<p>…eventually approaching the big old zero. In some way this is very
similar to the weighted average above, only the weights are dictated
by math, decaying uniformly. The smaller the starting weight, the
faster it approaches zero.</p>

<p>Only… there is a problem: weights do not add up to 1. The sum of
the first 3 numbers alone is already 2.439! (Exercise for the reader: what number
does the sum of the weights approach and why?)</p>

<p>What earned Poisson, Holts or Roberts a permanent place in the history
of Mathematics is solving this with a succinct and elegant formula:</p>

<script type="math/tex; mode=display">
\hat{y}_x = \alpha \cdot y_x + (1-\alpha) \cdot \hat{y}_{x-1} \\
</script>

<p>If you stare at it just long enough, you will see that the expected
value $\hat{y}_x$ is the sum of two products: $\alpha \cdot y_x$ and
$(1-\alpha) \cdot \hat{y}_{x-1}$. You can think of $\alpha$ (alpha)
as a sort of a starting weight 0.9 in the above (problematic)
example. It is called the <em>smoothing factor</em> or <em>smoothing
coefficient</em> (depending on who wrote your text book).</p>

<p>So essentially we’ve got a weighted moving average with two weights:
$\alpha$ and $1-\alpha$.  The sum of $\alpha$ and $1-\alpha$ is 1, so
all is well.</p>

<p>Now let’s zoom in on the right side of the sum. Cleverly, $1-\alpha$
is multiplied by the <em>previous</em> expected value
$\hat{y}_{x-1}$. Which, if you think about it, is the result of the
same formula, which makes the expression recursive (and programmers
love recursion), and if you were to write it all out on paper you would
quickly see that $(1-\alpha)$ is multiplied by itself again and again
all the way to beginning of the series, if there is one, infinitely
otherwise. And this is why this method is called
<em>exponential</em>.</p>

<p>Another important thing about $\alpha$ is that its value dictates how
much weight we give the most recent observed value versus the last
expected. It’s a kind of a lever that gives more weight to the left
side when it’s higher (closer to 1) or the right side when it’s lower
(closer to 0).</p>

<p>Perhaps $\alpha$ would be better referred to as <em>memory decay rate</em>: the
higher the $\alpha$, the faster the method “forgets”.</p>

<h4 id="why-is-it-called-smoothing">Why is it called “smoothing”?</h4>

<p>To the best of my understanding this simply refers to the effect these
methods have on a graph if you were to plot the values: jagged lines
become smoother.  Moving average also has the same effect, so it
deserves the right to be called smoothing just as well.</p>

<h4 id="implementation">Implementation</h4>

<p>There is an aspect of this method that programmers would appreciate
that is of no concern to mathematicians: it’s simple and efficient to
implement. Here is some Python. Unlike the previous examples, this
function returns expected values for the whole series, not just one
point.</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># given a series and alpha, return series of smoothed points</span>
</span><span class="line"><span class="k">def</span> <span class="nf">exponential_smoothing</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">series</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="c"># first value is same as series</span>
</span><span class="line">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)):</span>
</span><span class="line">        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">series</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">result</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">    <span class="k">return</span> <span class="n">result</span>
</span><span class="line">
</span><span class="line"><span class="c"># &gt;&gt;&gt; exponential_smoothing(series, 0.1)</span>
</span><span class="line"><span class="c"># [3, 3.7, 4.53, 5.377, 6.0393, 6.43537, 6.991833]</span>
</span><span class="line"><span class="c"># &gt;&gt;&gt; exponential_smoothing(series, 0.9)</span>
</span><span class="line"><span class="c"># [3, 9.3, 11.73, 12.873000000000001, 12.0873, 10.20873, 11.820873]</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The figure below shows exponentially smoothed version of our series
with $\alpha$ of 0.9 (red) and $\alpha$ of 0.1 (orange).</p>

<p><img src="/images/hw02.png" /></p>

<p>Looking at the above picture it is apparent that the $\alpha$ value of 0.9
follows the observed values much closer than 0.1. This isn’t going to
be true for any series, each series has its best $\alpha$ (or
several). The process of finding the best $\alpha$ is referred to as
<em>fitting</em> and we will discuss it later separately.</p>

<h2 id="quick-review">Quick Review</h2>

<p>We’ve learned some history, basic terminology (series and how it knows
no time, method, error SSE, MSE and fitting). And we’ve learned some
basic forecasting methods: naive, simple average, moving average,
weighted moving average and, finally, single exponential smoothing.</p>

<p>One very important characteristic of all of the above methods is that
remarkably, they can only forecast a <em>single</em> point. That’s correct,
just one.</p>

<p>In <a href="/blog/2016/02/16/triple-exponential-smoothing-forecasting-part-ii/">Part II</a> we will focus on methods that can forecast more than
one point.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Gregory Trubetskoy</span></span>

      








  


<time datetime="2016-01-29T15:36:00-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2016</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/" data-via="humblehack" data-counturl="http://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2015/09/23/storing-time-series-in-postgresql-efficiently/" title="Previous Post: Storing Time Series in PostgreSQL efficiently">&laquo; Storing Time Series in PostgreSQL efficiently</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/02/16/triple-exponential-smoothing-forecasting-part-ii/" title="Next Post: Holt-Winters Forecasting for Dummies - Part II">Holt-Winters Forecasting for Dummies - Part II &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    


<section>
  <br/>
  <p>
    <small>Send me some crypto if you like this content, TIA!</small><br/><br/>
<tt><small>Bitcoin: 1GrishapUx3mofmQuTbL4JEEMdzGgeSEcu</small></tt><br/>
<tt><small>Litecoin: LXLGrXXQM2t5ckq4gmznsVzpRNhRVD1V8w</small></tt><br/>
  </p>
</section>

<section>
  <h1>About Me</h1>

  <p>
  <div style="width: 75%; margin: 0 auto;">
  <a href="https://twitter.com/humblehack" class="twitter-follow-button"
    data-show-count="">Follow @humblehack</a>
  </div>
  </p>

  <p>I am currently a (Data) Hacker at <a href="http://voxmedia.com">Vox Media</a>.</p>
  <p>Grisha is a common Russian short name for Gregory. It is pronounced more like Greesha.</p>
  <p>Years ago I wrote <a href="http://modpython.org">mod_python</a>, which became a hugely succesful OSS Project and is still in use by millions of sites.</p>
  <p>I am a former VP and member emeritus of the <a href="http://apache.org">Apache Software Foundation</a>.</p>
  <p>I started programming professionally back when I was a teenager.
  I've spent most of my early career working at large ISP's solving industrial-scale hosting challenges. Since around 2009 I've become more intersted in and now work exclusively on data infrastuctre, both big and small, but mostly big.</p>
  <p>I was born and grew up in Moscow, Russia, though I've lived in the Washington, DC (USA) area for more than half of my life now. Our kids were born and go to school here, it's gradually become home for us.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/12/15/blockchain-and-postgres/">The Bitcoin Blockchain PostgresSQL Schema</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/20/blockchain-in-postgresql-part-2/">Blockchain in PostgreSQL Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/10/postgre-as-a-full-node/">Bitcoin Transaction Hash in Pure PostgreSQL</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/28/electricity-cost-of-1-bitcoin/">Electricity cost of 1 Bitcoin (Sep 2017)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/25/bitcoin-value-2/">Bitcoin: USD Value</a>
      </li>
    
  </ul>
</section>

<br/><br/><br/>
<br/><br/><br/>
<br/><br/><br/>

<section>
<script type="text/javascript"><!--
google_ad_client = "pub-9718360309690383";
google_ad_width = 200;
google_ad_height = 200;
google_ad_format = "200x200_as";
google_ad_type = "image";
//-->
</script>
<div style="text-align: center;">
  <div style="text-align: left; width: 200px; display: block; margin-left: auto; margin-right: auto;">
    <script type="text/javascript"
            src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
            </script>
  </div>
</div>

</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  <img src="https://www.ispol.com/grisha_org.gif" height="1" width="1">
  Copyright &copy; 2018 - Gregory Trubetskoy -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'grisha';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/';
        var disqus_url = 'http://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'https://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'https://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
