<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>mod_python performance part 2: high(er) concurrency | Gregory Trubetskoy</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Tl;dr
As is evident from the table below, mod_python 3.5
(in pre-release testing as of this writing) is currently the fastest tool when it
comes to running Python in your web server, and second-fastest as a
WSGI container.

  
    Server
    Version
    Req/s
    % of httpd static
    Notes
  
  
    nxweb static file
    3.2.0-dev
    512,767
     347.1 % 
    &#34;memcache&#34;:false. (626,270 if true)
  
  
    nginx static file
    1.0.15
    430,135
     291.1 %
    stock CentOS 6.3 rpm
  
  
    httpd static file
    2.4.4, mpm_event
    147,746
     100.0 % 
    
  
  
    mod_python handler
    3.5, Python 2.7.5
    125,139
     84.7 % 
    
  
  
    uWSGI
    1.9.18.2
    119,175
     80.7 % 
    -p 16 --threads 1
  
  
    mod_python wsgi
    3.5, Python 2.7.5
    87,304
     59.1 % 
    
  
  
    mod_wsgi
    3.4
    76,251
     51.6 % 
    embedded mode
  
  
    nxweb wsgi
    3.2.0-dev, Python 2.7.5
    15,141
     10.2 % 
    posibly misconfigured?
  

The point of this test
I wanted to see how mod_python compares to other tools of similar
purpose on high-end hardware and with relatively high concurrency. As
I&rsquo;ve written before
you&rsquo;d be foolish to base your platform decision on these numbers
because speed in this case matters very little. So the point of this
is just make sure that mod_python is in the ballpark with the rest and
that there isn&rsquo;t anything seriously wrong with it. And surprisingly,
mod_python is actually pretty fast, fastest, even, though in its own
category (a raw mod_python handler).">
    <meta name="generator" content="Hugo 0.146.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    <meta name="author" content="Gregory Trubetskoy">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    

    
      <link rel="canonical" href="https://grisha.org/blog/2013/11/07/mod-python-performance-revisited/">
    

    
    
    <meta property="og:url" content="https://grisha.org/blog/2013/11/07/mod-python-performance-revisited/">
  <meta property="og:site_name" content="Gregory Trubetskoy">
  <meta property="og:title" content="mod_python performance part 2: high(er) concurrency">
  <meta property="og:description" content="Tl;dr As is evident from the table below, mod_python 3.5 (in pre-release testing as of this writing) is currently the fastest tool when it comes to running Python in your web server, and second-fastest as a WSGI container.
Server Version Req/s % of httpd static Notes nxweb static file 3.2.0-dev 512,767 347.1 % &#34;memcache&#34;:false. (626,270 if true) nginx static file 1.0.15 430,135 291.1 % stock CentOS 6.3 rpm httpd static file 2.4.4, mpm_event 147,746 100.0 % mod_python handler 3.5, Python 2.7.5 125,139 84.7 % uWSGI 1.9.18.2 119,175 80.7 % -p 16 --threads 1 mod_python wsgi 3.5, Python 2.7.5 87,304 59.1 % mod_wsgi 3.4 76,251 51.6 % embedded mode nxweb wsgi 3.2.0-dev, Python 2.7.5 15,141 10.2 % posibly misconfigured? The point of this test I wanted to see how mod_python compares to other tools of similar purpose on high-end hardware and with relatively high concurrency. As I’ve written before you’d be foolish to base your platform decision on these numbers because speed in this case matters very little. So the point of this is just make sure that mod_python is in the ballpark with the rest and that there isn’t anything seriously wrong with it. And surprisingly, mod_python is actually pretty fast, fastest, even, though in its own category (a raw mod_python handler).">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2013-11-07T17:51:00+00:00">
    <meta property="article:modified_time" content="2013-11-07T17:51:00+00:00">

  <meta itemprop="name" content="mod_python performance part 2: high(er) concurrency">
  <meta itemprop="description" content="Tl;dr As is evident from the table below, mod_python 3.5 (in pre-release testing as of this writing) is currently the fastest tool when it comes to running Python in your web server, and second-fastest as a WSGI container.
Server Version Req/s % of httpd static Notes nxweb static file 3.2.0-dev 512,767 347.1 % &#34;memcache&#34;:false. (626,270 if true) nginx static file 1.0.15 430,135 291.1 % stock CentOS 6.3 rpm httpd static file 2.4.4, mpm_event 147,746 100.0 % mod_python handler 3.5, Python 2.7.5 125,139 84.7 % uWSGI 1.9.18.2 119,175 80.7 % -p 16 --threads 1 mod_python wsgi 3.5, Python 2.7.5 87,304 59.1 % mod_wsgi 3.4 76,251 51.6 % embedded mode nxweb wsgi 3.2.0-dev, Python 2.7.5 15,141 10.2 % posibly misconfigured? The point of this test I wanted to see how mod_python compares to other tools of similar purpose on high-end hardware and with relatively high concurrency. As I’ve written before you’d be foolish to base your platform decision on these numbers because speed in this case matters very little. So the point of this is just make sure that mod_python is in the ballpark with the rest and that there isn’t anything seriously wrong with it. And surprisingly, mod_python is actually pretty fast, fastest, even, though in its own category (a raw mod_python handler).">
  <meta itemprop="datePublished" content="2013-11-07T17:51:00+00:00">
  <meta itemprop="dateModified" content="2013-11-07T17:51:00+00:00">
  <meta itemprop="wordCount" content="1387">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="mod_python performance part 2: high(er) concurrency">
  <meta name="twitter:description" content="Tl;dr As is evident from the table below, mod_python 3.5 (in pre-release testing as of this writing) is currently the fastest tool when it comes to running Python in your web server, and second-fastest as a WSGI container.
Server Version Req/s % of httpd static Notes nxweb static file 3.2.0-dev 512,767 347.1 % &#34;memcache&#34;:false. (626,270 if true) nginx static file 1.0.15 430,135 291.1 % stock CentOS 6.3 rpm httpd static file 2.4.4, mpm_event 147,746 100.0 % mod_python handler 3.5, Python 2.7.5 125,139 84.7 % uWSGI 1.9.18.2 119,175 80.7 % -p 16 --threads 1 mod_python wsgi 3.5, Python 2.7.5 87,304 59.1 % mod_wsgi 3.4 76,251 51.6 % embedded mode nxweb wsgi 3.2.0-dev, Python 2.7.5 15,141 10.2 % posibly misconfigured? The point of this test I wanted to see how mod_python compares to other tools of similar purpose on high-end hardware and with relatively high concurrency. As I’ve written before you’d be foolish to base your platform decision on these numbers because speed in this case matters very little. So the point of this is just make sure that mod_python is in the ballpark with the rest and that there isn’t anything seriously wrong with it. And surprisingly, mod_python is actually pretty fast, fastest, even, though in its own category (a raw mod_python handler).">

      
      
    
	
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Gregory Trubetskoy
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/archives/" title="Archives page">
              Archives
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l mw7 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">mod_python performance part 2: high(er) concurrency</h1>
      
      <p class="tracked"><strong>Gregory Trubetskoy</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2013-11-07T17:51:00Z">November 7, 2013</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100-l"><h3 id="tldr">Tl;dr</h3>
<p>As is evident from the table below, mod_python <a href="https://github.com/grisha/mod_python/tree/3.5.x">3.5</a>
(in pre-release testing as of this writing) is currently the fastest tool when it
comes to running Python in your web server, and second-fastest as a
WSGI container.</p>
<table border="1">
  <tr>
    <th>Server</th>
    <th>Version</th>
    <th>Req/s</th>
    <th>% of httpd static</th>
    <th>Notes</th>
  </tr>
  <tr>
    <th><a href="https://bitbucket.org/yarosla/nxweb/wiki/Benchmarks">nxweb</a> static file</th>
    <td>3.2.0-dev</td>
    <td>512,767</td>
    <td> 347.1 % </td>
    <td>"memcache":false. (626,270 if true)</td>
  </tr>
  <tr>
    <th><a href="http://nginx.com/">nginx</a> static file</th>
    <td>1.0.15</td>
    <td>430,135</td>
    <td> 291.1 %</td>
    <td>stock CentOS 6.3 rpm</td>
  </tr>
  <tr>
    <th><a href="http://httpd.apache.org/">httpd</a> static file</th>
    <td>2.4.4, mpm_event</td>
    <td>147,746</td>
    <td> 100.0 % </td>
    <td></td>
  </tr>
  <tr>
    <th>mod_python <a href="http://modpython.org/live/current/doc-html/pythonapi.html#overview-of-a-request-handler">handler</a></th>
    <td>3.5, Python 2.7.5</td>
    <td>125,139</td>
    <td> 84.7 % </td>
    <td></td>
  </tr>
  <tr>
    <th><a href="https://uwsgi-docs.readthedocs.org/en/latest/">uWSGI</a></th>
    <td>1.9.18.2</td>
    <td>119,175</td>
    <td> 80.7 % </td>
    <td>-p 16 --threads 1</td>
  </tr>
  <tr>
    <th>mod_python <a href="http://modpython.org/live/current/doc-html/handlers.html#wsgi-handler">wsgi</a></th>
    <td>3.5, Python 2.7.5</td>
    <td>87,304</td>
    <td> 59.1 % </td>
    <td></td>
  </tr>
  <tr>
    <th><a href="http://code.google.com/p/modwsgi/">mod_wsgi</a></th>
    <td>3.4</td>
    <td>76,251</td>
    <td> 51.6 % </td>
    <td>embedded mode</td>
  </tr>
  <tr>
    <th>nxweb wsgi</th>
    <td>3.2.0-dev, Python 2.7.5</td>
    <td>15,141</td>
    <td> 10.2 % </td>
    <td>posibly misconfigured?</td>
  </tr>
</table>
<h2 id="the-point-of-this-test">The point of this test</h2>
<p>I wanted to see how mod_python compares to other tools of similar
purpose on high-end hardware and with relatively high concurrency. As
I&rsquo;ve <a href="http://grisha.org/blog/2013/10/10/mod-python-performance/">written before</a>
you&rsquo;d be foolish to base your platform decision on these numbers
because speed in this case matters very little. So the point of this
is just make sure that mod_python is in the ballpark with the rest and
that there isn&rsquo;t anything seriously wrong with it. And surprisingly,
mod_python is actually pretty fast, <em>fastest</em>, even, though in its own
category (a raw mod_python handler).</p>
<h2 id="test-rig">Test rig</h2>
<p>The server is a 24-core Intel Xeon 3GHz with 64GB RAM, running Linux
2.6.32 (CentOS 6.3).</p>
<p>The testing was done with
<a href="https://bitbucket.org/yarosla/httpress/wiki/Home">httpress</a>, which
was chosen after having tried
<a href="http://httpd.apache.org/docs/2.4/programs/ab.html">ab</a>,
<a href="http://www.hpl.hp.com/research/linux/httperf/">httperf</a> and
<a href="http://redmine.lighttpd.net/projects/weighttp/wiki">weighttp</a>. The exact command was:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>httpress -n 5000000 -c 120 -t 8 -k http://127.0.0.1/
</span></span></code></pre></div><p>Concurrency of 120 was chosen as the highest number I could run across
all setups without getting strange errors. &ldquo;Strange errors&rdquo; could be
disconnects, delays and stuck connections, all tunable by anything
from Linux kernel configuration to specific tool configs. I very much
wanted concurrency to be at least a few times higher but it quickly
became apparent that getting to that level would require very
significant system tweaking for which I just didn&rsquo;t have the time. 120
concurrent requests is nothing to sneeze at though: if you sustained
this rate for a day of python handler serving, you&rsquo;d have processed
10,812,009,600 requests (on a single server!).</p>
<p>I should also note that in my tweaking of various configurations I
couldn&rsquo;t get the requests/s numbers any significantly higher than what
you see above. Increasing concurrency and number of workers mostly
increased errors rather than r/s, which is also interesting because
it&rsquo;s important how gracefuly each of these tools fails, but failure
mode is a whole different subject.</p>
<p>The tests were done via the loopback (127.0.0.1) because having tried
hitting the server from outside it became apparent that the network
was the bottleneck.</p>
<p>Keepalives were in use (-k), which means that all of the 5 million
requests are processed over only about fifty thousand TCP
connections. Without keepalives this would be more of the Linux kernel
test because the bulk of the work establishing and taking down a
connection happens in the kernel.</p>
<p>Before running the 5 million requests I ran 100,000 as a &ldquo;warm up&rdquo;.</p>
<p>This post does not include the actual code for the WSGI app and mod_python handlers because it was same as
in my <a href="http://grisha.org/blog/2013/10/10/mod-python-performance/">last post on mod_python performance testing</a>.</p>
<h2 id="why-httpress">Why httpress</h2>
<p><a href="http://httpd.apache.org/docs/2.4/programs/ab.html">ab</a> simply can&rsquo;t run more than about 150K requests per second, so it
couldn&rsquo;t adequately test nxweb and nginx static file serving.</p>
<p><a href="http://www.hpl.hp.com/research/linux/httperf/">httperf</a> looked
promising at first, but as is <a href="http://gwan.com/en_apachebench_httperf.html">noted here</a> its requests per
second cannot be trusted because it gradually increases the
load.</p>
<p><a href="http://redmine.lighttpd.net/projects/weighttp/wiki">weighttp</a> seemed
good, but somehow got stuck on idle but not yet closed connections
which affected the request/s negatively.</p>
<p><a href="https://bitbucket.org/yarosla/httpress/wiki/Home">httpress</a> claimed that it &ldquo;promptly timeouts stucked connections,
forces all hanging connections to close after the main run, does not
allow hanging or interrupted connections to affect the measurement&rdquo;,
which is just what I needed. And it worked really great too.</p>
<h2 id="the-choice-of-contenders">The choice of contenders</h2>
<p>mod_python and mod_wsgi are the obvious choices, uWSGI/Nginx combo is
known as a low-resource and fast alternative. I came across nxweb
while looking at httpress (it&rsquo;s written by the same person
(<a href="https://bitbucket.org/yarosla">Yaroslav Stavnichiy</a>), it looks to be the
fastest (open source) web server currently out there, faster than (closed source)
G-WAN, even.</p>
<h2 id="specific-tool-notes">Specific tool notes</h2>
<p>The code used for testing and the configs were essentially same as what
I used in my <a href="http://grisha.org/blog/2013/10/10/mod-python-performance/">previous post on mod_python performance testing</a>.
The key differences are listed below.</p>
<h3 id="apache">Apache</h3>
<p>The key config on Apache was:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>ThreadsPerChild 25    # default
</span></span><span style="display:flex;"><span>StartServers 16
</span></span><span style="display:flex;"><span>MinSpareThreads 400
</span></span></code></pre></div><p>MinSpareThreads ensures that Apache starts all possible processes and
threads on startup (25 * 16 = 400) so that there is no ramp up
period and it&rsquo;s tsunami-ready right away.</p>
<h3 id="uwsgi">uWSGI</h3>
<p>The comparison with uWSGI isn&rsquo;t entriely appropriate because it was
running listening on a unix domain socket behind Nginx. The -p 16
&ndash;threads 1 (16 worker processes with a single thread each) was chosen
as the best performing option after some experimentation. Upping -p to
32 reduced r/s to 86233, 64 to 47296. Upping &ndash;threads to 2 (with 16
workers) reduced r/s to 55925 (by half, which is weird - mod_python has no
problems with 25 threads). &ndash;single-interpreter didn&rsquo;t seem to have
any significant impact.</p>
<p>The actual uWSGI command was:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>uwsgi -s logs/uwsgi.sock --pp htdocs  -M -p 16 --threads 1 -w mp_wsgi -z 30 -l 120 -L
</span></span></code></pre></div><p>A note on the uWSGI performance. Initially it seemed to be
outperforming the mod_python handler by nearly a factor of two. Then
after all kinds of puzzled head-scratching, I decided to verify that
every hit ran my Python code - I did this by writing a dot to a file
and making sure that the file size matches the number of hits in the
end. It turned out that about one third of the requests from Nginx to
uWSGI were erroring out, but httpress didn&rsquo;t see them as errors. So if
you&rsquo;re going to attempt to replicate this, watch out for this
condition. EDIT: Thanks to uWSGI&rsquo;s author Roberto De Loris&rsquo; help, it
turned out that this was a result of misconfiguration on my part - the
-l parameter should be set higher than 120. (This explains how I
arrived at 120 as the concurrency chosen for the test too). The
request/s number and uWSGI&rsquo;s position in my table is still correct.</p>
<h3 id="nginx">Nginx</h3>
<p>The relevant parts of the nginx config were (Note: this is not the
complete config for brevity):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>worker_processes 24;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>events {
</span></span><span style="display:flex;"><span>  worker_connections 1024;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>http {
</span></span><span style="display:flex;"><span>  server_tokens off;
</span></span><span style="display:flex;"><span>  keepalive_timeout 65;
</span></span><span style="display:flex;"><span>  sendfile on;
</span></span><span style="display:flex;"><span>  tcp_nopush on;
</span></span><span style="display:flex;"><span>  tcp_nodelay on;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  access_log /dev/null main;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>  upstream uwsgi {
</span></span><span style="display:flex;"><span>     ip_hash;
</span></span><span style="display:flex;"><span>     server unix:logs/uwsgi.sock;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><h3 id="conclusion">Conclusion</h3>
<p>Mod_python is plenty fast. Considering that unlike with other
contenders large parts of the code are written in Python and thus are
readable and debuggable by not just C programmers, it&rsquo;s quite a feat.</p>
<p>I was surprised by Apache&rsquo;s slow static file serving compared to Nginx
and Nxweb (the latter, although still young and in development seems like a
very cool web server).</p>
<p>On the other hand I am not all that convinced that the Nginx/uWSGI set
up is as cool as it is touted everywhere. Unquestionably Nginx is a
super solid server and Apache has some catching up to do when it comes
to acting as a static file server or a reverse proxy. But when it
comes to serving Python-generated content, my money would be on Apache
rather than uWSGI. The &ldquo;low&rdquo; 120 concurrency level for this test was
largely chosen because of uWSGI (Apache started going haywire on me at
about 400+ concurrent connections). EDIT: Thanks to Roberto&rsquo;s comment,
this turned out to be an error on my part (see comments). uWSGI can
handle higher concurrencies if -l is set higher.</p>
<p>It&rsquo;s also interesting that on my laptop a mod_python handler
outperformed the Apache static file, but it wasn&rsquo;t the case on the big
server.</p>
<p>I didn&rsquo;t do Python 3 testing, it would be interesting to see how much
difference it makes as well.</p>
<p>I realize this post may be missing key config data - I had to leave
out a lot because of time contraints (and my lazyness) - so if you see
any obvious gaps, please comment, I will try to address them.</p>
<p>P.S. Did I mention mod_python 3.5 supports Python 3? Please help
me <a href="https://github.com/grisha/mod_python/issues/9">test it</a>!</p>
<p>
<iframe src="http://ghbtns.com/github-btn.html?user=grisha&repo=mod_python&type=watch&count=true&size=large"
  allowtransparency="true" frameborder="0" scrolling="0" width="170" height="30"></iframe>
<iframe src="http://ghbtns.com/github-btn.html?user=grisha&repo=mod_python&type=fork&count=true&size=large"
  allowtransparency="true" frameborder="0" scrolling="0" width="170" height="30"></iframe>
<p><a href="https://twitter.com/mod_python" class="twitter-follow-button" data-show-count="false" data-size="large">Follow @mod_python</a></p>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
        
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "grisha" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
      </div>
    </div></article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="https://grisha.org/" >
    &copy;  Gregory Trubetskoy 2026 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
